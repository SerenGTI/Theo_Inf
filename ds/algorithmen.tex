%!TEX root = ../main.tex

\renewcommand{\time}{\operatorname{time}}
\newcommand{\avtime}{\operatorname{av-time}}
\renewcommand{\O}{\mathcal O}

\chapter{Laufzeitanalyse}
Zunächst benötigen wir einen Algorithmenbegriff.
\begin{definition}{Algorithmus}
	Ein Algorithmus ist eine schrittweise auszuführende Vorschrift, bei der jeder auszuführende Schritt tatsächlich in endlicher Zeit und auf eindeutig definierte Weise ausgeführt werden kann.
\end{definition}
Grundsätzlich soll vor Beginn der Ausführung immer eine Eingabe der Länge $n\in\N$ bereitgestellt sein, auf die die ausführende Maschine zugreifen kann.

\section{Worst- und Average-case Laufzeiten}
Analog zur Aufwandsanalyse von Maschinen aus Theoretischer Informatik 2 ist die Anzahl Berechnungsschritte, die für den gegebenen Algorithmus $A$ bei Input $x$ durchgeführt werden müssen gegeben durch die Abbildung
\begin{equation*}
 	\time_A(x):\Sigma^\ast \rightarrow \N\cup\simpleset{\infty}.
\end{equation*}
Hieraus ergibt sich die \emph{worst-case-Laufzeit} bei Eingabelänge höchstens $n$ als
\begin{equation*}
	\time_A(n)=\max_{|x|\leq n}\simpleset{\time_A(x)}.
\end{equation*}


Um die sogenannte \emph{average-case-Laufzeit} zu berechnen, gehen wir davon aus, dass alle Eingaben der Länge $n$ gleich wahrscheinlich sind (gleichverteilt).
Damit ist
\begin{equation*}
	\avtime_A(n)=\frac{1}{N}\sum_{|x|=n}\time_A(x)
\end{equation*}
wobei $N=|\set{x}{|x|=n}|$.


\section{Landau-Symbole}
Die fünf Landau-Symbole sind $\O, o, \Omega,\omega$ und $\Theta$, mit ihnen werden Klassen von Funktionen $\N\rightarrow\N$ beschrieben.

\begin{description}
	\item[$(\O)$] Man sagt $g$ ist zu $f$ eine asymptotische obere Schranke, wenn
	\begin{equation*}
		f\in\O(g)\enspace\Longleftrightarrow\enspace \limsup_{n\to\infty} \left|\frac{f(n)}{g(n)}\right|<\infty\enspace\Longleftrightarrow\enspace \ \exists c,N_0\in\N\ \forall n\geq N_0:f(n)\leq c*g(n)
	\end{equation*}

	\item[$(\Omega)$] Analog zu $\O$ stellt $\Omega$ die Klasse der asymptotischen unteren Schranken dar.
	\begin{equation*}
		f\in\Omega(g) \enspace\Longleftrightarrow\enspace \liminf_{n\to\infty} \left|\frac{f(n)}{g(n)}\right|>0\enspace\Longleftrightarrow\enspace \ \exists c,N_0\in\N\ \forall n\geq N_0:f(n)\geq c*g(n)
	\end{equation*}


	\item[$(\Theta)$] Man sagt $g$ ist zu $f$ eine asymptotisch scharfe Schranke, wenn
	\begin{equation*}
		f\in\Theta(g)\enspace\Longleftrightarrow\enspace f\in\Theta(g)\wedge f\in\O(g)
	\end{equation*}
	Es gilt $\Theta(g)=\O(g)\cap \Omega(g)$.


	\item[$(o)$] $f$ ist gegenüber $g$ asymptotisch vernachlässigbar, wenn
	\begin{equation*}
		f\in o(g)\enspace\Longleftrightarrow\enspace \lim_{n\to\infty} \left|\frac{f(n)}{g(n)}\right|=0\enspace\Longleftrightarrow\enspace \ \forall c\ \exists N_0\in\N\ \forall n\geq N_0:f(n)< c*g(n)
	\end{equation*}
	Man sieht, dass $o(g)\subset \O(g)$ gilt.


	\item[$(\omega)$] $f$ ist gegenüber $g$ asymptotisch dominant, wenn
	\begin{equation*}
		f\in\omega(g) \enspace\Longleftrightarrow\enspace \lim_{n\to\infty} \left|\frac{f(n)}{g(n)}\right|=\infty\enspace\Longleftrightarrow\enspace \ \forall c\ \exists N_0\in\N\ \forall n\geq N_0:f(n)\geq c*g(n)
	\end{equation*}
	Analog ist hier wieder $\omega(g)\subset \Omega(g)$.
\end{description}


\section{Laufzeit von rekursiven Programmen}
Rekursion bedeutet, ein Problem für den Parameterwert $n$ unter Zuhilfenahme des Ergebnisses für den/die Parameterwert/e $m$ bzw $m_i$ zu berechnen.
Oft ist dabei $m<n$. Man reduziert also ein Problem auf ein bereits gelöstes plus einen meist nur noch kleinen Rechenaufwand.


Um Aussagen über die Laufzeit zu treffen, muss man sogenannte Rekursionsgleichungen lösen.

Entspricht die Rekursionsgleichung einem der beiden folgenden Sonderfälle, so können die sogenannten Masterheoreme angewandt werden.
\begin{satz}{Mastertheorem 1}
	Für $a,b\in\N$, $b>1$ und eine Funktion $g:\N\rightarrow \N$ mit $g\in\Theta(n^c)$ gelte
	\begin{align*}
		t(1)&=g(1)\\
		t(n)&=a*t\left(\frac nb\right)+g(n)
	\end{align*}
	Dann gilt 
	\begin{equation*}
		t(n)\in\begin{cases}
		\Theta(n^c)&\text{falls }a<b^c\\
		\Theta(n^c\log n)&\text{falls }a=b^c\\
		\Theta(n^{\frac{\log a}{\log b}})&\text{falls }a>b^c\\
		\end{cases}
	\end{equation*}
\end{satz}

\begin{satz}{Mastertheorem 2}
	Sei $r>0$ und die Zahlen $\alpha_1\geq 0$ für alle $i$ und erfüllen $\sum_{i=1}^r\alpha_i<1$.
	Wenn die Rekursive Funktion $t$ die Ungleichung
	\begin{equation*}
		t(n)\leq \left( \sum_{i=1}^r t(\lceil \alpha_i*n\rceil) \right)+c*n
	\end{equation*}
	für ein $c>0$ erfüllt, dann ist $t(n)\in\O(n)$.
\end{satz}


\chapter{Entwurfsmethoden}
\section{Divide and Conquer}
Das divide and Conquer / Teile und herrsche-Prinzip basiert auf Rekursion. Allgemein wird die Eingabe in zwei (oder mehr) Teile aufgeteilt. Die Berechnung wird rekursiv auf den Teilen durchgeführt, am Schluss werden die beiden Teillösungen zur Gesamtlösung zusammengeführt.

Wichtig für einen effizienten divide and conquer-Algorithmus ist, dass die Teile ungefähr gleich groß sind, d.h. $m\approx \frac n2$. Außerdem sollte das Aufteilen und Zusammensetzen mit linearer Zeit ($\O(n)$ bzw. $\Theta(n)$) auskommen.

\paragraph{Beispiele:}
\begin{itemize}
	\item Mergesort
	\item Quicksort
	\item Multiplikation großer Zahlen
\end{itemize}

\section{Dynamisches Programmieren}
\section{Backtracking bzw. Branch and Bound}
Backtracking -> Tiefensuche

Mache Schritte solange möglich. Falls das Ergebnis keine Lösung ist, gehe einen zurück und nimm den nächsten Pfad.


Branch and Bound ist eine Sonderform des Backtracking. Anhand vom Bound wird der nächte Pfad ausgewählt, so kommmt man möglicherweise früher zum Ergebnis.

\section{Greedy Algorithmen}
\section{Randomisierte Algorithmen}

\section{Andere Algorithmen}
\subsection{Schnelle Multiplikation}
Wir wollen $X*Y$ berechnen.

Wir setzen $X=A*b^n+B$ und $Y=C*b^n+D$ damit wäre dann
\begin{equation*}
	XY=AC*b^{2n}+(AD+BC)*b^n+BD
\end{equation*}
Dies würde normalerweise 4 Multiplikationen benötigen. Allerdings kann man mit einem kleinen Trick nur drei Produkte berechnen
\begin{align*}
	P_1&=AC\\
	P_2&=BD\\
	P_3&=(A+B)(C+D)=AC+AD+BC+BD
\end{align*}
damit kann man aus den drei Produkten den fehlenden Teil durch Subtraktion berechnen
\begin{equation*}
	P_3-P_1-P_2=(AD+BC).
\end{equation*}

\subsubsection{Medianberechnung in Linearzeit}
\subsubsection{Bottom Up Heapsort und ultimatives Heapsort}
$1,4n\log n$ und dann $1n\log n$, wow!





